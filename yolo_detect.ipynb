{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from collections import OrderedDict\n",
    "import PIL\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import Tensor\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to use CPU or GPU\n",
    "CUDA = True\n",
    "use_cuda = torch.cuda.is_available() # Use cuda if possible (can manually set to True of False if needed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "print('CUDA is %savailable. Using device: %s'%(('not ' if use_cuda == False else ''),device))\n",
    "\n",
    "weight_path = './yolov3.weights'\n",
    "coco_path = './coco.names'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download weight file, around 300mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the values of all the nodes in a pretrained network (Darknet53.conv.74)\n",
    "If the weights request throws an SSL error on MacOS, uncomment the line below and run again.\"\"\"\n",
    "#!/Applications/Python\\ 3.6/Install\\ Certificates.command \n",
    "\n",
    "import urllib.request\n",
    "if not osp.exists(weight_path):\n",
    "    urllib.request.urlretrieve('https://pjreddie.com/media/files/yolov3.weights', weight_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco(path):\n",
    "    # Loads the list of names of objects that the network knows how to classify\n",
    "    \n",
    "    with open(path) as f:\n",
    "        return [line.rstrip(\"\\n\") for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = load_coco(coco_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_transforms(inner_dim, outer_dim):\n",
    "    \"\"\"Calculates the aspect ratio of an image, and determines where the \n",
    "    centre moves to when the images is scaled.\"\"\"\n",
    "    \n",
    "    outer_w, outer_h = outer_dim\n",
    "    inner_w, inner_h = inner_dim\n",
    "    ratio = min(outer_w / inner_w, outer_h / inner_h)\n",
    "    box_w = int(inner_w * ratio)\n",
    "    box_h = int(inner_h * ratio)\n",
    "    box_x_offset = (outer_w // 2) - (box_w // 2)\n",
    "    box_y_offset = (outer_h // 2) - (box_h // 2)\n",
    "    return box_w, box_h, box_x_offset, box_y_offset, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_image(img, dim):\n",
    "    \"\"\"Create the background, by looking at the image through a predefined 'letterbox', \n",
    "    just big enough for the image to fit into. Any extra space is left black.\"\"\"\n",
    "\n",
    "    image = np.full(dim +(3,), 128)\n",
    "        \n",
    "    img_dim = (img.shape[1], img.shape[0])\n",
    "    box_w, box_h, box_x, box_y, ratio = letterbox_transforms(img_dim, dim)\n",
    "    box_image = cv2.resize(img, (box_w,box_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "    #Put the box image on top of the blank image\n",
    "    image[box_y:box_y+box_h, box_x:box_x+box_w] = box_image\n",
    "        \n",
    "    return image, (box_w, box_h, box_x, box_y, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode - letterbox, resize\n",
    "def load_image(img_path, mode=None, dim=None):\n",
    "    \"\"\"Grabs an image and resizes\"\"\"\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    trans = None\n",
    "    if mode is not None and dim is not None:\n",
    "        if mode == 'letterbox':\n",
    "            img, trans = letterbox_image(img, dim)\n",
    "        elif mode == 'resize':\n",
    "            img = cv2.resize(img, dim)\n",
    "    \n",
    "    img = torch.from_numpy(img).float().permute(2,0,1) / 255\n",
    "    return img, trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic network building blocks - conv_bn_relu, res_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_relu(nn.Module):\n",
    "    \"\"\"Creates a layer than performs:\n",
    "        - convolutions (patch that passes over an image)\n",
    "        - batch normalisation (turns the output of the nodes into a z-score,\n",
    "                               to prevent some becoming too important)\n",
    "        - rectified linear units (output=input*weight+constant, but if output is negative, output=0\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout, ks, s=1, pad='SAME', padding=0, bn=True, act=\"leakyRelu\"):\n",
    "        \"\"\"Inputs: Number of connections for the layer in, number out, kernel size, kernel strides, \n",
    "        padding mode, padding, batch normalisation, activation function.\"\"\"\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn = bn\n",
    "        self.act = act\n",
    "                \n",
    "        if pad == 'SAME':\n",
    "            padding = (ks - 1) // 2\n",
    "            \n",
    "        self.conv = nn.Conv2d(nin, nout, ks, s, padding, bias=not bn)\n",
    "        if bn == True:\n",
    "            self.bn = nn.BatchNorm2d(nout)\n",
    "        if act == \"leakyRelu\":\n",
    "            self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "    \n",
    "class res_layer(nn.Module):\n",
    "    \"\"\"A layer that makes connections to layers that are not beside each other,\n",
    "    by skipping layers between. This allows the network to retain skills from previous layers better\"\"\"\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_bn_relu(nin, nin//2, ks=1)\n",
    "        self.conv2 = conv_bn_relu(nin//2, nin, ks=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map2cfgDict - used  to creating mapping that follows the cfg file from prjreddit's repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map2cfgDict(mlist):\n",
    "    \"\"\" Goes through the configuration file of original darknet, \n",
    "    recording all the parameters and skipping descriptions of layers\"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "    mdict = OrderedDict()\n",
    "    for i,m in enumerate(mlist):\n",
    "        if isinstance(m, res_layer):\n",
    "            mdict[idx] = None\n",
    "            mdict[idx+1] = None\n",
    "            idx += 2\n",
    "        mdict[idx] = i\n",
    "        idx += 1\n",
    "    return mdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet53 - Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_res_stack(nin, num_blk):\n",
    "    \"\"\"Creates a list accessible by different modules. The list contains n (num_blk) layers, \n",
    "    each with i (nin) inputs. The kernel size is 3x3 and kernel stride length 2.\n",
    "    They have twice as many output as input connections, and a residual layer is created and attached \n",
    "    to each of the layers before adding the next layer.\"\"\" \n",
    "    \n",
    "    return nn.ModuleList([conv_bn_relu(nin, nin*2, 3, s=2)] \\\n",
    "           + [res_layer(nin*2) for n in range(num_blk)])\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"Creates a neural network object based on the number of layers you want, and how many \n",
    "    output connections each layer will have. Darknet(1,2,8,8,4) means:\n",
    "        - 1 layer with each node having 1*32 input connections, followed by\n",
    "        - 2 layers with each node having 2*32 input connections, followed by\n",
    "        - 8 layers with each node having 4*32 input connections, followed by\n",
    "        - 8 layers with each node having 8*32 input connections, followed by\n",
    "        - 4 layers with each node having 16*32 input connections.\n",
    "    Each layer has an associate residual layer tacked after it.\"\"\"\n",
    "\n",
    "    def __init__(self, blkList, nout=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Make an empty list of layers\n",
    "        self.mlist = nn.ModuleList()\n",
    "        \n",
    "        # Add a first layer with 3 input connections and 32 outputs and kernel size 3.\n",
    "        self.mlist += [conv_bn_relu(3, nout, 3)]\n",
    "        \n",
    "        \"\"\"For every layer requested, add it to the list. \n",
    "            - Input connections = 32 * (2 for 1st layer, 2 for 2nd, 4 for 3rd, 8 for 4th, etc.)\n",
    "            - Output connectins = number requested\"\"\"\n",
    "        for i,nb in enumerate(blkList):\n",
    "            self.mlist += make_res_stack(nout*(2**i), nb)\n",
    "        \n",
    "        # Uses the darknet configuration file for the network parameters\n",
    "        self.map2yolocfg = map2cfgDict(self.mlist)\n",
    "        self.cachedOutDict = dict()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i,m in enumerate(self.mlist):\n",
    "            x = m(x)\n",
    "            if i in self.cachedOutDict:\n",
    "                self.cachedOutDict[i] = x\n",
    "        return x\n",
    "    \n",
    "    #mode - normal  -- direct index to mlist\n",
    "    #     - yolocfg -- index follow the sequences of the cfg file from https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
    "    def addCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idxs = self.map2yolocfg[idx]\n",
    "        self.cachedOutDict[idxs] = None\n",
    "        \n",
    "    def getCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idxs = self.map2yolocfg[idx]\n",
    "        return self.cachedOutDict[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreDetectionConvGroup - conv layers before the yolo detection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreDetectionConvGroup(nn.Module):\n",
    "    \"\"\"Create convolutional layers to classify an image as having or not having certain objects.\n",
    "    This happens at different zoom levels, and is always followed by a yolo layer that tries to \n",
    "    find where in the frame they are located.\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout, num_conv=3):\n",
    "        super().__init__()\n",
    "        self.mlist = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_conv):\n",
    "            self.mlist += [conv_bn_relu(nin, nout, ks=1)]\n",
    "            self.mlist += [conv_bn_relu(nout, nout*2, ks=3)]\n",
    "            if i == 0:\n",
    "                nin = nout*2\n",
    "                \n",
    "        self.mlist += [nn.Conv2d(nin, 255, 1)]\n",
    "        self.map2yolocfg = map2cfgDict(self.mlist)\n",
    "        self.cachedOutDict = dict()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i,m in enumerate(self.mlist):\n",
    "            x = m(x)\n",
    "            if i in self.cachedOutDict:\n",
    "                self.cachedOutDict[i] = x\n",
    "        return x\n",
    "    \n",
    "    #mode - normal  -- direct index to mlist \n",
    "    #     - yolocfg -- index follow the sequences of the cfg file from https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
    "    def addCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idx = self.getIdxFromYoloIdx(idx)\n",
    "        elif idx < 0:\n",
    "            idx = len(self.mlist) - idx\n",
    "        \n",
    "        self.cachedOutDict[idx] = None\n",
    "        \n",
    "    def getCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idx = self.getIdxFromYoloIdx(idx)\n",
    "        elif idx < 0:\n",
    "            idx = len(self.mlist) - idx\n",
    "        return self.cachedOutDict[idx]\n",
    "    \n",
    "    def getIdxFromYoloIdx(self,idx):\n",
    "        if idx < 0:\n",
    "            return len(self.map2yolocfg) + idx\n",
    "        else:\n",
    "            return self.map2yolocfg[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UpsampleGroup - grab feature maps from early high resolution layers and concatenate it with upsampled feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleGroup(nn.Module):\n",
    "    \"\"\"Takes a low resolution object detector, increases the size of the image, but retains\n",
    "    any features already detected at the high resolution.\"\"\"\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.conv = conv_bn_relu(nin, nin//2, ks=1)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        \n",
    "    def forward(self, route_head, route_tail):\n",
    "        out = self.up(self.conv(route_head))\n",
    "        return torch.cat((out, route_tail), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo Detection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLayer(nn.Module):\n",
    "    \"\"\"Creates an object that will create bounding boxes around where it believes objects are. \n",
    "    Three bounding boxes are predicted at different points on the image called anchors.\n",
    "    The number of anchors is predefined. \n",
    "    Each bounding box can predict multiple objects at different probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, anchors, numClass):\n",
    "        super().__init__()\n",
    "        self.anchors = torch.Tensor(anchors).to(device)\n",
    "        self.numClass = numClass\n",
    "        \n",
    "        #self.mlist = nn.ModuleList()\n",
    "        #self.det = [conv_bn_relu(3, 4, 3)]\n",
    "        #self.mlist += self.det\n",
    "        \n",
    "    def forward(self, x, img_dim):\n",
    "        grid_size = x.shape[-1]\n",
    "        numBBoxAttrib = 5 + self.numClass\n",
    "        numAnchors = len(self.anchors)\n",
    "        stride = img_dim // grid_size\n",
    "        \n",
    "        #Reshape the feature map from [batch x channel x grid_w x grid_h] to [batch x boundingbox x box attributes]\n",
    "        x = x.permute(0,2,3,1).contiguous().view(-1, grid_size*grid_size*numAnchors, numBBoxAttrib)\n",
    "        \n",
    "        #Sigmoid tx,ty,to\n",
    "        x[:,:,:2] = x[:,:,:2].sigmoid()\n",
    "        x[:,:,4] = x[:,:,4].sigmoid()\n",
    "        \n",
    "        #Add cx,cy to sig(tx),sig(ty) , create cx,cy creating meshgrid of \"grid_size\"\n",
    "        x_offset = torch.arange(grid_size).view(1,-1,1,1).repeat(grid_size,1,numAnchors,1).view(grid_size*grid_size*numAnchors, 1)\n",
    "        y_offset = torch.arange(grid_size).view(-1,1,1,1).repeat(1,grid_size,numAnchors,1).view(grid_size*grid_size*numAnchors, 1)\n",
    "        mesh = torch.cat((x_offset,y_offset), 1)\n",
    "        x[:,:,:2] = x[:,:,:2].add(mesh.to(device))\n",
    "        \n",
    "        #Rescale anchors to fit the stride, multiply it by e^(tw) and e^(th)\n",
    "        x[:,:,2:4] = torch.exp(x[:,:,2:4]).mul(self.anchors.div(stride).repeat(grid_size**2,1))\n",
    "        \n",
    "        #Sigmoid class scores\n",
    "        x[:,:,5:] = x[:,:,5:].sigmoid()\n",
    "        \n",
    "        #Rescale bx,by,bw,bh by stride to orginal image size\n",
    "        x[:,:,:4] *= stride\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire network - putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloNet(nn.Module):\n",
    "    \"\"\"Creates an object which contains all the necessary network design features.\"\"\"\n",
    "    \n",
    "    def __init__(self, anchors = [10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326]):\n",
    "        super().__init__()\n",
    "        nin = 32\n",
    "        numClass = 80\n",
    "        \n",
    "        # Groups the anchors into pairs (10,13), (16,30), etc.\n",
    "        anchors = [(anchors[i], anchors[i+1]) for i in range(0,len(anchors),2)]   \n",
    "        \n",
    "        \"\"\"Groups the anchor pairs into three groups: \n",
    "            - largest [(116, 90), (156, 198), (373, 326)]\n",
    "            - middle [(30, 61), (62, 45), (59, 119)]\n",
    "            - smallest [(10, 13), (16, 30), (33, 23)]\"\"\"        \n",
    "        anchors = [anchors[i:i+3] for i in range(0, len(anchors), 3)][::-1]\n",
    "                \n",
    "        \"\"\"Create darknet object, with 5 sets of layers, each set containing (1,2,8, etc.) copies of \n",
    "        that type of layer. The sets deeper in have many more connections than the earlier ones.\"\"\"\n",
    "        self.feature = Darknet([1,2,8,8,4])\n",
    "        self.feature.addCachedOut(61)\n",
    "        self.feature.addCachedOut(36)\n",
    "        \n",
    "        # The next three sections look at low zoom, medium zoom and high zoom aspects of the image.\n",
    "\n",
    "        # (Low zoom). Create convolutional layers with many (1024 input, 512 output) connections per node.\n",
    "        self.pre_det1 = PreDetectionConvGroup(1024, 512)\n",
    "        # Try to place boxes on objects big anchor boxes\n",
    "        self.yolo1 = YoloLayer(anchors[0], numClass)\n",
    "        # Fetch output from 4th layer backward including yolo layer\n",
    "        self.pre_det1.addCachedOut(-3) \n",
    "        \n",
    "        # Zoom in to medium power\n",
    "        self.up1 = UpsampleGroup(512)\n",
    "        # Create convolutional layers with medium number of (768 input, 256 output) connections per node.\n",
    "        self.pre_det2 = PreDetectionConvGroup(768, 256)\n",
    "        # Try to place boxes on objects medium anchor boxes\n",
    "        self.yolo2 = YoloLayer(anchors[1], numClass)\n",
    "        self.pre_det2.addCachedOut(-3)\n",
    "        \n",
    "        # Zoom in to high power\n",
    "        self.up2 = UpsampleGroup(256)\n",
    "        # Create convolutional layers with fewer (384 input, 128 output) connections per node.\n",
    "        self.pre_det3 = PreDetectionConvGroup(384, 128)\n",
    "        # Try to place boxes on objects small anchor boxes\n",
    "        self.yolo3 = YoloLayer(anchors[2], numClass)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"This function executes the layer updates in the correct order.\"\"\"\n",
    "        \n",
    "        #Get image dimension\n",
    "        img_size = x.shape[-1]\n",
    "        \n",
    "        #Extract features\n",
    "        out = self.feature(x)\n",
    "                \n",
    "        #Detection layer 1\n",
    "        out = self.pre_det1(out)\n",
    "        det1 = self.yolo1(out, img_size)\n",
    "        \n",
    "        #Upsample 1\n",
    "        r_head1 = self.pre_det1.getCachedOut(-3)\n",
    "        r_tail1 = self.feature.getCachedOut(61)\n",
    "        out = self.up1(r_head1,r_tail1)\n",
    "                \n",
    "        #Detection layer 2\n",
    "        out = self.pre_det2(out)\n",
    "        det2 = self.yolo2(out, img_size)\n",
    "        \n",
    "        #Upsample 2\n",
    "        r_head2 = self.pre_det2.getCachedOut(-3)\n",
    "        r_tail2 = self.feature.getCachedOut(36)\n",
    "        out = self.up2(r_head2,r_tail2)\n",
    "                \n",
    "        #Detection layer 3\n",
    "        out = self.pre_det3(out)\n",
    "        det3 = self.yolo3(out, img_size)\n",
    "        \n",
    "        return det1,det2,det3\n",
    "    \n",
    "    def loadWeight(self, weights_path):\n",
    "        # Load the starting values (the weights and biases) for all the nodes in the whole network.\n",
    "        \n",
    "        wm = WeightManager(self)\n",
    "        wm.loadWeight(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeightManager - load weight file from official Yolo site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightManager:\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.conv_list = self.find_conv_layers(model)\n",
    "\n",
    "    def loadWeight(self, weight_path):\n",
    "        ptr = 0\n",
    "        weights = self.read_file(weight_path)\n",
    "        print(len(weights))\n",
    "        for m in self.conv_list:\n",
    "            if type(m) == conv_bn_relu:\n",
    "                ptr = self.load_conv_bn_relu(m, weights, ptr)\n",
    "            elif type(m) == nn.Conv2d:\n",
    "                ptr = self.load_conv2D(m, weights, ptr)\n",
    "        return ptr\n",
    "                \n",
    "    def read_file(self, file):\n",
    "        with open(file, \"rb\") as fp:\n",
    "            header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "            self.header = torch.from_numpy(header)\n",
    "            self.seen = self.header[3]\n",
    "            weights = np.fromfile(fp, dtype = np.float32)\n",
    "        return weights\n",
    "    \n",
    "    def copy_weight_to_model_parameters(self, param, weights, ptr):\n",
    "        num_el = param.numel()\n",
    "        param.data.copy_(torch.from_numpy(weights[ptr:ptr + num_el])\n",
    "                             .view_as(param.data))\n",
    "        return ptr + num_el\n",
    "    \n",
    "    def load_conv_bn_relu(self, m, weights, ptr):\n",
    "        ptr = self.copy_weight_to_model_parameters(m.bn.bias, weights, ptr)\n",
    "        ptr = self.copy_weight_to_model_parameters(m.bn.weight, weights, ptr)\n",
    "        ptr = self.copy_weight_to_model_parameters(m.bn.running_mean, weights, ptr)\n",
    "        ptr = self.copy_weight_to_model_parameters(m.bn.running_var, weights, ptr)\n",
    "        ptr = self.copy_weight_to_model_parameters(m.conv.weight, weights, ptr)\n",
    "        return ptr\n",
    "        \n",
    "    def load_conv2D(self, m, weights, ptr):\n",
    "        ptr = self.copy_weight_to_model_parameters(m.bias, weights, ptr)\n",
    "        ptr = self.copy_weight_to_model_parameters(m.weight, weights, ptr)\n",
    "        return ptr\n",
    "        \n",
    "    def find_conv_layers(self, mod):\n",
    "        module_list = []\n",
    "        for m in mod.children():\n",
    "            if type(m) == conv_bn_relu:\n",
    "                module_list += [m]\n",
    "            elif type(m) == nn.Conv2d:\n",
    "                module_list += [m]\n",
    "            elif isinstance(m, (nn.ModuleList, nn.Module)):\n",
    "                module_list += self.find_conv_layers(m)\n",
    "            elif type(m) == res_layer:\n",
    "                module_list += self.find_conv_layers(m)\n",
    "        return module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = YoloNet().to(device).eval()\n",
    "net.loadWeight(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imglist = os.listdir('imgs')\n",
    "for i,img in enumerate(imglist):\n",
    "    print('{}: {}'.format(i,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"imgs/{imglist[0]}\"\n",
    "resolution = 416\n",
    "testData, trans = load_image(img_path, mode='letterbox', dim=(resolution, resolution))\n",
    "testData = testData.unsqueeze(0).to(device)\n",
    "det1,det2,det3 = net(testData)\n",
    "\n",
    "det1.shape,det2.shape,det3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions to find unique values in tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_unique(inp, CUDA=True):\n",
    "    if CUDA:\n",
    "        inp_cpu = inp.detach().cpu()\n",
    "    \n",
    "    res_cpu = torch.unique(inp_cpu)\n",
    "    res = inp.new(res_cpu.shape)\n",
    "    res.copy_(res_cpu)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unqiue_with_order(inp, CUDA=True):\n",
    "    if CUDA:\n",
    "        inp_np = inp.detach().cpu().numpy()\n",
    "    \n",
    "    _, idx = np.unique(inp, return_index=True)\n",
    "    result = inp_np[np.sort(idx)]\n",
    "    result_tensor = torch.from_numpy(result)\n",
    "    res = inp.new(result_tensor.shape)\n",
    "    res.copy_(result_tensor)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU and non-max supression(NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_vectorized(bbox):\n",
    "    num_box = bbox.shape[0]\n",
    "    \n",
    "    bbox_leftTop_x =  bbox[:,0]\n",
    "    bbox_leftTop_y =  bbox[:,1]\n",
    "    bbox_rightBottom_x = bbox[:,2]\n",
    "    bbox_rightBottom_y = bbox[:,3]\n",
    "    \n",
    "    inter_leftTop_x     =  torch.max(bbox_leftTop_x.unsqueeze(1).repeat(1,num_box), bbox_leftTop_x)\n",
    "    inter_leftTop_y     =  torch.max(bbox_leftTop_y.unsqueeze(1).repeat(1,num_box), bbox_leftTop_y)\n",
    "    inter_rightBottom_x =  torch.min(bbox_rightBottom_x.unsqueeze(1).repeat(1,num_box), bbox_rightBottom_x)\n",
    "    inter_rightBottom_y =  torch.min(bbox_rightBottom_y.unsqueeze(1).repeat(1,num_box), bbox_rightBottom_y)\n",
    "    \n",
    "    inter_area = torch.clamp(inter_rightBottom_x - inter_leftTop_x, min=0) * torch.clamp(inter_rightBottom_y - inter_leftTop_y, min=0)\n",
    "    bbox_area = (bbox_rightBottom_x - bbox_leftTop_x) * (bbox_rightBottom_y - bbox_leftTop_y)\n",
    "    union_area = bbox_area.expand(num_box,-1) + bbox_area.expand(num_box,-1).transpose(0, 1) - inter_area\n",
    "    \n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "#Iterate through the bounding boxes and remove rows accordingly\n",
    "def reduce_row_by_column(inp):\n",
    "    i = 0\n",
    "    while i < inp.shape[0]:\n",
    "        remove_row_idx = inp[i][1].item()\n",
    "        if inp[i][0] != remove_row_idx and i < inp.shape[0]:\n",
    "            keep_mask = (inp[:,0] != remove_row_idx).nonzero().squeeze()\n",
    "            inp = inp[keep_mask]\n",
    "        i += 1\n",
    "    return inp\n",
    "\n",
    "#bbox is expected to be sorted by class score in descending order\n",
    "def nms(bbox, iou, nms_thres):\n",
    "    #Create a mapping that indicates which row has iou > threshold\n",
    "    remove_map = (iou > nms_thres).nonzero()\n",
    "    remove_map = reduce_row_by_column(remove_map)\n",
    "    \n",
    "    remove_idx = torch_unique(remove_map[:,0])\n",
    "    res_bbox = bbox[remove_idx]\n",
    "    \n",
    "    return res_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing - convert predictions from network to bounding boxes (calls IOU/NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(detections, num_classes, obj_conf_thr=0.5, nms_thr=0.4):\n",
    "    #Zero bounding box with objectioness confidence score less than threshold \n",
    "    obj_conf_filter = (detections[:,:,4] > obj_conf_thr).float().unsqueeze(2)\n",
    "    detections = detections * obj_conf_filter\n",
    "           \n",
    "    #Transform bounding box coordinates to two corners\n",
    "    box = detections.new(detections[:,:,:4].shape)\n",
    "    box[:,:,0] = detections[:,:,0] - detections[:,:,2]/2\n",
    "    box[:,:,1] = detections[:,:,1] - detections[:,:,3]/2\n",
    "    box[:,:,2] = box[:,:,0] + detections[:,:,2]\n",
    "    box[:,:,3] = box[:,:,1] + detections[:,:,3]\n",
    "    detections[:,:,:4] = box\n",
    "    \n",
    "    num_batches = detections.shape[0]\n",
    "    results = torch.Tensor().to(device)\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "        batch_results = torch.Tensor().to(device)\n",
    "        img_det = detections[b]\n",
    "        \n",
    "        \n",
    "        max_class_score, max_class_idx= torch.max(img_det[:,5:5 + num_classes], 1)\n",
    "        img_det = torch.cat((img_det[:,:5],\n",
    "                             max_class_score.float().unsqueeze(1),\n",
    "                             max_class_idx.float().unsqueeze(1)\n",
    "                            ), 1)\n",
    "        #img det - [b1_x, b1_y, b2_x, b2_y, obj_conf, class_score, class]\n",
    "        \n",
    "        #Remove zeroed rows\n",
    "        nonzero_idx =  img_det[:,4].nonzero()\n",
    "        img_det = img_det[nonzero_idx,:].view(-1,7)\n",
    "        \n",
    "        if img_det.shape[0] != 0:\n",
    "            #Get the classes\n",
    "            img_classes = torch_unique(img_det[:,-1])\n",
    "            for c in img_classes:\n",
    "                # Select rows with \"c\" class and sort by the class score\n",
    "                class_img_det = img_det[(img_det[:,-1] == c).nonzero().squeeze()]\n",
    "                # If there is only one detection, it will return a 1D tensor. Therefore, we perform a view to keep it in 2D\n",
    "                class_img_det = class_img_det.view(-1, 7)\n",
    "                #Sort by objectness score\n",
    "                _, sort_idx = class_img_det[:,4].sort(descending=True)\n",
    "                class_img_det = class_img_det[sort_idx]\n",
    "\n",
    "                iou = iou_vectorized(class_img_det)\n",
    "                #Alert: There's another loop operation in nms function\n",
    "                class_img_det = nms(class_img_det, iou, nms_thr)\n",
    "                batch_results = torch.cat((batch_results, class_img_det), 0)\n",
    "\n",
    "            num_dets = batch_results.shape[0]\n",
    "            batch_col = results.new_full((num_dets, 1), b)\n",
    "            batch_results = torch.cat((batch_col, batch_results), 1)\n",
    "            results = torch.cat((results, batch_results), 0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = torch.cat((det1,det2,det3), 1)\n",
    "pdet = postprocessing(detections, 80, obj_conf_thr=0.5)\n",
    "detections.shape, pdet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdet.shape)\n",
    "pdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw image and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rect(ax, b):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=0.5))\n",
    "    draw_outline(patch, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(ax, xy, txt, sz=14):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_plt(box):\n",
    "    bx, by = box[0], box[1]\n",
    "    bw = box[2] - box[0]\n",
    "    bh = box[3] - box[1]\n",
    "    return [bx, by, bw, bh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output(img, dets):\n",
    "    ax = show_img(img, figsize=(16,8))\n",
    "    for d in dets:\n",
    "        draw_rect(ax, bbox_plt(d))\n",
    "        c = classes[d[-1].int().item()]\n",
    "        draw_text(ax, d[:2], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions with the image processed by the network (letterboxed image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output(testData.squeeze(0).permute(1,2,0), pdet[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform bounding boxes to fit the original image's dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the original image with no letterbox\n",
    "img, _ = load_image(f\"imgs/{imglist[0]}\")\n",
    "img = img.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform(box, x_max, y_max, x_offset, y_offset, ratio):\n",
    "    box[:,[0,2]] = torch.clamp((box[:,[0,2]] - x_offset) / ratio, 0, x_max)\n",
    "    box[:,[1,3]] = torch.clamp((box[:,[1,3]] - y_offset) / ratio, 0, y_max)\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trans is returned from the load_image call when we first input the letterboxed image into the network\n",
    "box = pdet[:, 1:5]\n",
    "box = bbox_transform(box, img.shape[1], img.shape[0], *trans[-3:])\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output(img, pdet[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    # Mode - None, 'letterbox', 'resize'\n",
    "    def __init__(self, data_dir, resize_dim, mode=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.resize_dim = resize_dim\n",
    "        self.mode = mode\n",
    "        self.data_files_list = [f for f in os.listdir(f'{self.data_dir}') if os.path.isfile(os.path.join(f'{self.data_dir}', f))]\n",
    "        self.trans_dict = dict()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(f'{self.data_dir}', self.data_files_list[idx])\n",
    "        image, trans = load_image(image_path, mode=self.mode, dim=self.resize_dim)\n",
    "        if self.mode == 'letterbox':\n",
    "            self.trans_dict[idx] = trans\n",
    "        return image\n",
    "    \n",
    "    @property\n",
    "    def filenames(self):\n",
    "        return self.data_files_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "img_folder = f\"imgs/\"\n",
    "resize_dim = (416, 416)\n",
    "\n",
    "yolo_ds = YoloDataset(img_folder, resize_dim, mode='letterbox')\n",
    "yolo_dl = DataLoader(yolo_ds, batch_size=bs, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader):\n",
    "    with torch.no_grad():\n",
    "        res = Tensor().to(device)\n",
    "        running_len = 0\n",
    "        for inp in dataloader:\n",
    "            inp = inp.to(device)\n",
    "\n",
    "            det1,det2,det3 = net(inp)\n",
    "            dets = torch.cat((det1,det2,det3), 1)\n",
    "            post_dets = postprocessing(dets, 80, obj_conf_thr=0.5)\n",
    "\n",
    "            post_dets[:,0] = post_dets[:,0] + running_len\n",
    "            res = torch.cat((res, post_dets), 0)\n",
    "            running_len += inp.shape[0]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = YoloNet().to(device).eval()\n",
    "net.loadWeight(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict(net, yolo_dl)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_ds.mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output_from_ds(ds, dets, idx):\n",
    "    img = ds[idx].permute(1,2,0)\n",
    "    trans = ds.trans_dict[idx]\n",
    "    row_dets = (dets[:,0] == idx).nonzero().squeeze()\n",
    "    img_dets = dets[row_dets, 1:].view(-1,7).clone()\n",
    "    \n",
    "    if img_dets.shape[0] != 0:\n",
    "        box = img_dets[:,:4]\n",
    "        box = bbox_transform(box, img.shape[1], img.shape[0], *trans[-3:])\n",
    "    \n",
    "    show_output(img, img_dets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the last argument to see different images placed under /img folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_output_from_ds(yolo_ds, res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imglist = os.listdir('imgs')\n",
    "for i,img in enumerate(imglist):\n",
    "    print('{}: {}'.format(i,img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
