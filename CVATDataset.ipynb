{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "from lxml import etree\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import Tensor\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from draw import show_img\n",
    "from utils import bbox_x1y1x2y2_to_xywh, bbox_x1y1x2y2_to_cxcywh, bbox_cxcywh_to_x1y1x2y2, bbox_cxcywh_to_xywh\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_transforms(inner_dim, outer_dim):\n",
    "    outer_w, outer_h = outer_dim\n",
    "    inner_w, inner_h = inner_dim\n",
    "    ratio = min(outer_w / inner_w, outer_h / inner_h)\n",
    "    box_w = int(inner_w * ratio)\n",
    "    box_h = int(inner_h * ratio)\n",
    "    box_x_offset = (outer_w // 2) - (box_w // 2)\n",
    "    box_y_offset = (outer_h // 2) - (box_h // 2)\n",
    "    return box_w, box_h, box_x_offset, box_y_offset, ratio\n",
    "\n",
    "def letterbox_image(img, dim):\n",
    "    #Create the background\n",
    "    image = np.full(dim +(3,), 128)\n",
    "        \n",
    "    img_dim = (img.shape[1], img.shape[0])\n",
    "    box_w, box_h, box_x, box_y, ratio = letterbox_transforms(img_dim, dim)\n",
    "    box_image = cv2.resize(img, (box_w,box_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "    #Put the box image on top of the blank image\n",
    "    image[box_y:box_y+box_h, box_x:box_x+box_w] = box_image\n",
    "    \n",
    "    transform = Tensor([box_w, box_h, box_x, box_y, ratio])\n",
    "    return image, transform\n",
    "\n",
    "# Mode - letterbox, resize\n",
    "def load_image(img_path, mode=None, dim=None):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_org_dim = (img.shape[1], img.shape[0])\n",
    "    trans = None\n",
    "    if mode is not None and dim is not None:\n",
    "        if mode == 'letterbox':\n",
    "            img, trans = letterbox_image(img, dim)\n",
    "        elif mode == 'resize':\n",
    "            img = cv2.resize(img, dim)\n",
    "    \n",
    "    img = torch.from_numpy(img).float().permute(2,0,1) / 255\n",
    "    return img, img_org_dim, trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_label(label, transform, dim):\n",
    "    label_x_offset = transform[..., 2] / dim[0]\n",
    "    label_y_offset = transform[..., 3] / dim[1]\n",
    "    box_w_ratio = transform[..., 0] / dim[0]\n",
    "    box_h_ratio = transform[..., 1] / dim[1]\n",
    "    label[..., [0,2]] = label[..., [0,2]] * box_w_ratio \n",
    "    label[..., [1,3]] = label[..., [1,3]] * box_h_ratio\n",
    "    label[..., 0] = label[..., 0] + label_x_offset\n",
    "    label[..., 1] = label[..., 1] + label_y_offset\n",
    "    return label\n",
    "\n",
    "def letterbox_label_reverse(label, transform, dim):\n",
    "    label_x_offset = transform[..., 2] / dim[0]\n",
    "    label_y_offset = transform[..., 3] / dim[1]\n",
    "    box_w_ratio = transform[..., 0] / dim[0]\n",
    "    box_h_ratio = transform[..., 1] / dim[1]\n",
    "    label[..., 0] = label[..., 0] - label_x_offset\n",
    "    label[..., 1] = label[..., 1] - label_y_offset\n",
    "    label[..., [0,2]] = torch.clamp(label[..., [0,2]] / box_w_ratio, 0, 1) \n",
    "    label[..., [1,3]] = torch.clamp(label[..., [1,3]] / box_h_ratio, 0, 1)\n",
    "    return label\n",
    "\n",
    "def fill_label_np_tensor(label, row, col):\n",
    "    label_tmp = np.full((row, col), 0.0)\n",
    "    if label is not None:\n",
    "        length = label.shape[0] if label.shape[0] < row else row\n",
    "        label_tmp[:length] = label[:length]\n",
    "    return label_tmp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_labels(xml_path):\n",
    "    labels = OrderedDict()\n",
    "    \n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot() \n",
    "    \n",
    "    img_tags = root.xpath(\"image\")\n",
    "\n",
    "    for image in img_tags:\n",
    "        img = image.get('name', None)\n",
    "        labels[img] = []\n",
    "        for box in image:\n",
    "            cls = box.get('label', None)\n",
    "            x1 = box.get('xtl', None)\n",
    "            y1 = box.get('ytl', None)\n",
    "            x2 = box.get('xbr', None)\n",
    "            y2 = box.get('ybr', None)\n",
    "            labels[img] += [{'cls' : cls, \n",
    "                             'x1'  : x1 ,\n",
    "                             'y1'  : y1 ,\n",
    "                             'x2'  : x2 ,\n",
    "                             'y2'  : y2  }]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVATDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_xml_path, dim=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_xml_path = label_xml_path\n",
    "        self.xml_dict = list(get_xml_labels(self.label_xml_path).items())\n",
    "        self.dim = dim\n",
    "        self.class2id = { 'x_wing': 0, 'tie': 1}\n",
    "        self.id2class = {v:k for k,v in self.class2id.items()}\n",
    "        self.is_train = True\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xml_dict)\n",
    "    \n",
    "    def isTrain(self, is_train):\n",
    "        self.is_train = is_train\n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.__getitem_train(idx)\n",
    "        else:\n",
    "            return self.__getitem_eval(idx)\n",
    "    \n",
    "    def __getitem_eval(self, idx):\n",
    "        img_path, label = self.xml_dict[idx]\n",
    "        img_path = osp.join(self.img_dir, img_path)\n",
    "        if osp.exists(img_path):\n",
    "            img, img_org_dim, trans = load_image(img_path, mode=None, dim=self.dim)\n",
    "        return img\n",
    "    \n",
    "    def __getitem_train(self, idx):\n",
    "        label = None\n",
    "        img_path, label = self.xml_dict[idx]\n",
    "        \n",
    "        img_path = osp.join(self.img_dir, img_path)\n",
    "        if osp.exists(img_path):\n",
    "            org_img = cv2.imread(img_path)\n",
    "            org_img = cv2.cvtColor(org_img, cv2.COLOR_BGR2RGB)\n",
    "            letterbox_img, transform = letterbox_image(org_img, self.dim)\n",
    "            \n",
    "            org_img = torch.from_numpy(org_img).float().permute(2,0,1) / 255\n",
    "            letterbox_img = torch.from_numpy(letterbox_img).float().permute(2,0,1) / 255\n",
    "        \n",
    "        org_w, org_h = org_img.shape[2], org_img.shape[1]\n",
    "        label = torch.from_numpy(np.array( [ [self.class2id[l['cls']],\n",
    "                                             l['x1'],\n",
    "                                             l['y1'],\n",
    "                                             l['x2'],\n",
    "                                             l['y2'] ] for l in label] ).astype(np.float))\n",
    "        \n",
    "        label_bbox = label[..., 1:5]\n",
    "        label_bbox[..., [0,2]] /= org_w\n",
    "        label_bbox[..., [1,3]] /= org_h\n",
    "        label_bbox = bbox_x1y1x2y2_to_cxcywh(label_bbox)\n",
    "\n",
    "        label = label.double()\n",
    "        transform = transform.double()\n",
    "        \n",
    "        if label is not None:\n",
    "            label_bbox = letterbox_label(label_bbox, transform, self.dim)\n",
    "\n",
    "        label = fill_label_np_tensor(label, 50, 5)\n",
    "        label = torch.from_numpy(label)\n",
    "        \n",
    "        sample = { 'org_img': org_img,\n",
    "                   'letterbox_img': letterbox_img,\n",
    "                   'transform': transform,\n",
    "                   'label': label,\n",
    "                   'img_path': img_path}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_drawTextWithBkgd(img, text, bt_left_pt, color, max_x, max_y, font_scale=2.0, font=cv2.FONT_HERSHEY_PLAIN):\n",
    "    (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, fontScale=font_scale, thickness=1)[0]\n",
    "        \n",
    "    t_pt1 = np.clip(bt_left_pt[0], 0, max_x - text_width), np.clip(bt_left_pt[1], text_height, max_y) \n",
    "    t_pt2 = t_pt1[0] + text_width, t_pt1[1] - text_height\n",
    "    \n",
    "    img = cv2.rectangle(img, t_pt1, t_pt2, color, cv2.FILLED, 4)\n",
    "    img = cv2.putText(img, text, t_pt1, cv2.FONT_HERSHEY_PLAIN, fontScale=font_scale, color=(0, 0, 0), thickness=2);\n",
    "    return img\n",
    "\n",
    "def get_color_pallete(num_color):\n",
    "    cmap = plt.get_cmap('tab20b')\n",
    "    colors = Tensor([cmap(i) for i in np.linspace(0, 1, num_color)])\n",
    "    bbox_colors = colors[torch.randperm(num_color)]\n",
    "    return bbox_colors\n",
    "\n",
    "def test_dataset(dataloader, output_dir, classes_names):\n",
    "    font_scale = 2.0\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    bbox_colors = get_color_pallete(20)\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for batch, sample in enumerate(train_dl):\n",
    "            if batch != 16:\n",
    "                continue\n",
    "            \n",
    "            for img, transform, labels, img_path in zip(sample['org_img'], sample['transform'], sample['label'], sample['img_path']):\n",
    "                img = np.ascontiguousarray(img.numpy().transpose(1, 2, 0))\n",
    "                img_w, img_h = img.shape[1], img.shape[0]\n",
    "\n",
    "                for l in labels:\n",
    "                    if l.sum() == 0:\n",
    "                        break;\n",
    "                        \n",
    "                    box_coord = l[1:5]\n",
    "                    cls = l[0]\n",
    "                    \n",
    "                    cls_text = classes_names[cls.int().item()]\n",
    "                    cls_color = bbox_colors[cls.int()].numpy().tolist()\n",
    "                    \n",
    "                    box_coord = letterbox_label_reverse(box_coord, transform, train_ds.dim)\n",
    "                    box_coord = bbox_cxcywh_to_x1y1x2y2(box_coord)\n",
    "                    box_coord[[0,2]] = box_coord[[0,2]] * img_w\n",
    "                    box_coord[[1,3]] = box_coord[[1,3]] * img_h\n",
    "\n",
    "                    pt1 = tuple(box_coord[0:2].int().numpy().tolist())\n",
    "                    pt2 = tuple(box_coord[2:4].int().numpy().tolist())\n",
    "                    \n",
    "                    img = cv2.rectangle(img, pt1, pt2, cls_color, 4)\n",
    "                    img = cv2_drawTextWithBkgd(img, cls_text, pt1, cls_color, img_w, img_h, font_scale=font_scale, font=font)\n",
    "                    \n",
    "                img = (img * 255).astype(np.int)\n",
    "                show_img(img, figsize=(16,8))\n",
    "                cv2.imwrite(osp.join(output_dir, os.path.basename(img_path)), img)\n",
    "                #break\n",
    "                 \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 416\n",
    "bs = 2\n",
    "dim = (sz, sz)\n",
    "cvat_img_dir = './custom_data/x_wing'\n",
    "label_xml_path = './custom_data/7_x_wing.xml'\n",
    "output_dir = './output'\n",
    "classes_path = './x_wing.names'\n",
    "classes_names = ['x_wing', 'tie']\n",
    "\n",
    "train_ds = CVATDataset(cvat_img_dir, label_xml_path, dim=dim)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset(train_dl, output_dir, classes_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around to get bounding boxes to be transformed in step with images.\n",
    "# If boxes are loaded as imgaug objects, then you can apply the transforms to them also.\n",
    "\n",
    "def get_sample_boxes(image):\n",
    "    # Function to get bounding box coordinates\n",
    "    xml_sample = image['label'][0]\n",
    "    x_dimension,y_dimension,channels = (sample['org_img'][0].permute(1,2,0).numpy() * 255).shape\n",
    "    simple_box_coordinates = []\n",
    "    box_objects = []\n",
    "    for box in xml_sample: \n",
    "        # if there are values for the bbox entry\n",
    "        if torch.sum(box) != 0:\n",
    "            print('original xml data\\n\\t',box)\n",
    "            # Box coordinates as fractions of the frame dimensions\n",
    "            x_1,y_1,x_2,y_2 = box[1],box[2],box[3],box[4]\n",
    "\n",
    "            # Box coordinates as pixels\n",
    "            print('image dimensions',x_dimension,y_dimension)\n",
    "            x_1 = int(x_dimension*x_1.item())        \n",
    "            y_1 = int(y_dimension*y_1.item()) \n",
    "            x_2 = int(x_dimension*x_2.item()) \n",
    "            y_2 = int(y_dimension*y_2.item()) \n",
    "            \n",
    "            # Send the simple box coordinates to the list\n",
    "            next_simple_box_coordinates = [x_1,y_1,x_2,y_2]\n",
    "            simple_box_coordinates.append(next_simple_box_coordinates)\n",
    "            \n",
    "            # Convert the box to an imgaug object and add it to the list\n",
    "            next_box_object = ia.BoundingBox(x1=x_1,y1=y_1,x2=x_2,y2=y_2)\n",
    "            box_objects.append(next_box_object)\n",
    "    print('box objects',box_objects)\n",
    "    print('\\nsimple box coordinates\\n\\t', simple_box_coordinates)\n",
    "    \n",
    "    # Create imgaug object for the bounding boxes\n",
    "    bbs = ia.BoundingBoxesOnImage(box_objects,shape=(x_dimension,y_dimension))\n",
    "    \n",
    "    \n",
    "    return simple_box_coordinates, bbs\n",
    "\n",
    "\n",
    "def show_sample_image(image, boxes=None):\n",
    "    # Function to display a test image\n",
    "    img = sample['org_img'][0].permute(1,2,0).numpy() * 255\n",
    "    img = img.astype(np.uint8).copy()\n",
    "    print('target window',img.shape)\n",
    "    \n",
    "    # Add bounding boxes as test to ensure they are correct\n",
    "    if boxes is not None:\n",
    "        for box_coord in boxes:            \n",
    "            pt1 = (box_coord[0],box_coord[1])#x1,y1\n",
    "            pt2 = (box_coord[2],box_coord[3]) #x2,y2\n",
    "                    \n",
    "            img = cv2.rectangle(img, pt1, pt2, (0,255,0), 4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    show_img(img / 255, figsize=(16,8))\n",
    "    print('image shape:',img.shape)\n",
    "    return img\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "sample = next(iter(train_dl))  \n",
    "box_array, box_ia_object = get_sample_boxes(sample)\n",
    "img = show_sample_image(sample, boxes = box_array)\n",
    "\n",
    "# -------------------TODO-------------------\n",
    "# Need to fix the coordinate ordering which looks jumbled\n",
    "\n",
    "# Need to have the sequencer modify the bounding boxes (see commented out lines below)\n",
    "\n",
    "# Add transformations to the list of augmentations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(img,bounding_boxes_on_image_object):\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    seq = iaa.Sequential(\n",
    "        [\n",
    "            # Blur each image with varying strength using\n",
    "            # gaussian blur (sigma between 0 and 3.0),\n",
    "            # average/uniform blur (kernel size between 2x2 and 7x7)\n",
    "            # median blur (kernel size between 3x3 and 11x11).\n",
    "            iaa.OneOf([\n",
    "                iaa.GaussianBlur((0, 3.0)),\n",
    "                iaa.AverageBlur(k=(2, 7)),\n",
    "                iaa.MedianBlur(k=(3, 11)),\n",
    "            ]),\n",
    "            # Sharpen each image, overlay the result with the original\n",
    "            # image using an alpha between 0 (no sharpening) and 1\n",
    "            # (full sharpening effect).\n",
    "            sometimes(iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5))),\n",
    "            # Add gaussian noise to some images.\n",
    "            sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)),\n",
    "            # Add a value of -5 to 5 to each pixel.\n",
    "            sometimes(iaa.Add((-5, 5), per_channel=0.5)),\n",
    "            # Change brightness of images (80-120% of original value).\n",
    "            sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.5)),\n",
    "            # Improve or worsen the contrast of images.\n",
    "            sometimes(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)),\n",
    "        ],\n",
    "        # do all of the above augmentations in random order\n",
    "        random_order=True\n",
    "    )\n",
    "    seq_det = seq.to_deterministic()\n",
    "    # augment images\n",
    "    img = seq_det.augment_images([img])[0]\n",
    "    # augment boxes\n",
    "    bbs_aug = seq_det.augment_bounding_boxes([bounding_boxes_on_image_object])[0]\n",
    "    \n",
    "    \n",
    "    return img,bbs_aug\n",
    "\n",
    "\n",
    "augmented_image,augmented_boxes = aug(img,box_ia_object)\n",
    "\n",
    "#pre-augmentation\n",
    "show_img(img / 255, figsize=(16,8))\n",
    "\n",
    "# post-augmentation\n",
    "image_after = augmented_boxes.draw_on_image(augmented_image, thickness=2,color=[0,0,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "seq = iaa.Sequential([iaa.Fliplr(0.5), iaa.GaussianBlur((0, 3.0)), iaa.MedianBlur(k=(3, 11))])\n",
    "\n",
    "# show an image with 8*8 augmented versions of image 0\n",
    "#seq.show_grid(images[0], cols=8, rows=8)\n",
    "\n",
    "# Show an image with 8*8 augmented versions of image 0 and 8*8 augmented\n",
    "# versions of image 1. The identical augmentations will be applied to\n",
    "# image 0 and 1.\n",
    "#seq.show_grid([images[0], images[1]], cols=8, rows=8)\n",
    "\n",
    "print(images[0].shape)\n",
    "\n",
    "show_img(images[0],  figsize=(16,8)) \n",
    "show_img(images[1],  figsize=(16,8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "seq = iaa.Sequential([ iaa.MedianBlur(k=(3, 11))])\n",
    "\n",
    "seq_det = seq.to_deterministic()\n",
    "images = seq_det.augment_images(images)\n",
    "\n",
    "show_img(images[2],  figsize=(16,8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code used in actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms=[]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def add(self, transform):\n",
    "        self.transforms.append(transform)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, max_objects=50, is_debug=False):\n",
    "        self.max_objects = max_objects\n",
    "        self.is_debug = is_debug\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['label']\n",
    "        if self.is_debug == False:\n",
    "            image = image.astype(np.float32)\n",
    "            image /= 255.0\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            image = image.astype(np.float32)\n",
    "\n",
    "        filled_labels = np.zeros((self.max_objects, 5), np.float32)\n",
    "        filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n",
    "        return {'image': torch.from_numpy(image), 'label': torch.from_numpy(filled_labels)}\n",
    "\n",
    "class KeepAspect(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        dim_diff = np.abs(h - w)\n",
    "        # Upper (left) and lower (right) padding\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        image_new = np.pad(image, pad, 'constant', constant_values=128)\n",
    "        padded_h, padded_w, _ = image_new.shape\n",
    "\n",
    "        # Extract coordinates for unpadded + unscaled image\n",
    "        x1 = w * (label[:, 1] - label[:, 3]/2)\n",
    "        y1 = h * (label[:, 2] - label[:, 4]/2)\n",
    "        x2 = w * (label[:, 1] + label[:, 3]/2)\n",
    "        y2 = h * (label[:, 2] + label[:, 4]/2)\n",
    "        # Adjust for added padding\n",
    "        x1 += pad[1][0]\n",
    "        y1 += pad[0][0]\n",
    "        x2 += pad[1][0]\n",
    "        y2 += pad[0][0]\n",
    "        # Calculate ratios from coordinates\n",
    "        label[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "        label[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "        label[:, 3] *= w / padded_w\n",
    "        label[:, 4] *= h / padded_h\n",
    "\n",
    "        return {'image': image_new, 'label': label}\n",
    "\n",
    "class ResizeImage(object):\n",
    "    def __init__(self, new_size, interpolation=cv2.INTER_LINEAR):\n",
    "        self.new_size = tuple(new_size) #  (w, h)\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = cv2.resize(image, self.new_size, interpolation=self.interpolation)\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class ImageBaseAug(object):\n",
    "    def __init__(self):\n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "        self.seq = iaa.Sequential(\n",
    "            [\n",
    "                # Blur each image with varying strength using\n",
    "                # gaussian blur (sigma between 0 and 3.0),\n",
    "                # average/uniform blur (kernel size between 2x2 and 7x7)\n",
    "                # median blur (kernel size between 3x3 and 11x11).\n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 3.0)),\n",
    "                    iaa.AverageBlur(k=(2, 7)),\n",
    "                    iaa.MedianBlur(k=(3, 11)),\n",
    "                ]),\n",
    "                # Sharpen each image, overlay the result with the original\n",
    "                # image using an alpha between 0 (no sharpening) and 1\n",
    "                # (full sharpening effect).\n",
    "                sometimes(iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5))),\n",
    "                # Add gaussian noise to some images.\n",
    "                sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)),\n",
    "                # Add a value of -5 to 5 to each pixel.\n",
    "                sometimes(iaa.Add((-5, 5), per_channel=0.5)),\n",
    "                # Change brightness of images (80-120% of original value).\n",
    "                sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.5)),\n",
    "                # Improve or worsen the contrast of images.\n",
    "                sometimes(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)),\n",
    "            ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        seq_det = self.seq.to_deterministic()\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = seq_det.augment_images([image])[0]\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
